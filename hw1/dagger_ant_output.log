Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
/root/miniconda3/envs/cs224r/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/root/miniconda3/envs/cs224r/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/root/miniconda3/envs/cs224r/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
Loading expert policy from... cs224r/policies/experts/Ant.pkl
obs (1, 111) (1, 111)
Done restoring expert policy...
########################
Logging outputs to  /root/autodl-tmp/cs224r/hw1/cs224r/scripts/../../data/q2_dagger_ant_Ant-v4_03-01-2026_13-06-44
########################
Using GPU id 0


********** Iteration 0 ************

Collecting data to be used for training...
Loading expert data from cs224r/expert_data/expert_data_Ant-v4.pkl

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4644.2568359375
Eval_StdReturn : 98.1924819946289
Eval_MaxReturn : 4833.00927734375
Eval_MinReturn : 4550.67236328125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4713.6533203125
Train_StdReturn : 12.196533203125
Train_MaxReturn : 4725.849609375
Train_MinReturn : 4701.45654296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 2000
TimeSinceStart : 8.30599331855774
Training Loss : -1.959841251373291
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4720.43994140625
Eval_StdReturn : 60.66276550292969
Eval_MaxReturn : 4838.67578125
Eval_MinReturn : 4671.34765625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4529.81103515625
Train_StdReturn : 0.0
Train_MaxReturn : 4529.81103515625
Train_MinReturn : 4529.81103515625
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 3000
TimeSinceStart : 17.52338433265686
Training Loss : -2.0732290744781494
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4542.97802734375
Eval_StdReturn : 81.39131927490234
Eval_MaxReturn : 4653.29931640625
Eval_MinReturn : 4403.3662109375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4648.3466796875
Train_StdReturn : 0.0
Train_MaxReturn : 4648.3466796875
Train_MinReturn : 4648.3466796875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 4000
TimeSinceStart : 26.443517923355103
Training Loss : -2.0829596519470215
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4729.96044921875
Eval_StdReturn : 99.22077941894531
Eval_MaxReturn : 4846.72998046875
Eval_MinReturn : 4575.03369140625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4762.267578125
Train_StdReturn : 0.0
Train_MaxReturn : 4762.267578125
Train_MinReturn : 4762.267578125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 5000
TimeSinceStart : 35.55134868621826
Training Loss : -2.1497294902801514
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4616.5908203125
Eval_StdReturn : 154.24400329589844
Eval_MaxReturn : 4731.376953125
Eval_MinReturn : 4321.95068359375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4700.857421875
Train_StdReturn : 0.0
Train_MaxReturn : 4700.857421875
Train_MinReturn : 4700.857421875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 6000
TimeSinceStart : 44.74561309814453
Training Loss : -2.1240251064300537
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4723.3916015625
Eval_StdReturn : 139.19090270996094
Eval_MaxReturn : 4918.0322265625
Eval_MinReturn : 4506.26123046875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 3797.21630859375
Train_StdReturn : 978.26708984375
Train_MaxReturn : 4775.4833984375
Train_MinReturn : 2818.94921875
Train_AverageEpLen : 804.0
Train_EnvstepsSoFar : 7608
TimeSinceStart : 54.61267590522766
Training Loss : -2.234156847000122
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4739.51318359375
Eval_StdReturn : 59.707393646240234
Eval_MaxReturn : 4822.54296875
Eval_MinReturn : 4641.3291015625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4842.263671875
Train_StdReturn : 0.0
Train_MaxReturn : 4842.263671875
Train_MinReturn : 4842.263671875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 8608
TimeSinceStart : 63.726072788238525
Training Loss : -2.173689603805542
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4736.005859375
Eval_StdReturn : 150.128662109375
Eval_MaxReturn : 4943.23779296875
Eval_MinReturn : 4561.1708984375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4848.1064453125
Train_StdReturn : 0.0
Train_MaxReturn : 4848.1064453125
Train_MinReturn : 4848.1064453125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 9608
TimeSinceStart : 72.87679529190063
Training Loss : -2.2312698364257812
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4678.70751953125
Eval_StdReturn : 104.65782928466797
Eval_MaxReturn : 4856.10791015625
Eval_MinReturn : 4538.607421875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4701.333984375
Train_StdReturn : 0.0
Train_MaxReturn : 4701.333984375
Train_MinReturn : 4701.333984375
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 10608
TimeSinceStart : 82.09403944015503
Training Loss : -2.1184298992156982
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4783.03271484375
Eval_StdReturn : 82.61355590820312
Eval_MaxReturn : 4876.4296875
Eval_MinReturn : 4657.0048828125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4713.5830078125
Train_StdReturn : 0.0
Train_MaxReturn : 4713.5830078125
Train_MinReturn : 4713.5830078125
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 11608
TimeSinceStart : 91.27411890029907
Training Loss : -2.2271652221679688
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...


