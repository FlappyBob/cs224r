# run_hw1.py å‚æ•°è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Š `run_hw1.py` ä¸­æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°çš„å«ä¹‰ã€ç”¨é€”å’Œæ¨èå€¼ã€‚

---

## ğŸ“‹ å‚æ•°åˆ†ç±»

### 1. å¿…éœ€å‚æ•°ï¼ˆRequired Parametersï¼‰

#### `--expert_policy_file` / `-epf`
```python
parser.add_argument('--expert_policy_file', '-epf', type=str, required=True)
```

**å«ä¹‰**ï¼šä¸“å®¶ç­–ç•¥æ–‡ä»¶çš„è·¯å¾„

**ä½œç”¨**ï¼š
- æŒ‡å®šé¢„è®­ç»ƒçš„ä¸“å®¶ç­–ç•¥æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ `.pkl` æ–‡ä»¶ï¼‰
- åœ¨ Behavior Cloning ä¸­ï¼Œè¿™ä¸ªæ–‡ä»¶ç”¨äºåŠ è½½ä¸“å®¶ç­–ç•¥ï¼Œç”¨äºï¼š
  - è¯„ä¼°ï¼šæ¯”è¾ƒå­¦ä¹ åˆ°çš„ç­–ç•¥ä¸ä¸“å®¶çš„æ€§èƒ½
  - DAggerï¼šåœ¨é‡æ–°æ ‡æ³¨æ—¶ä½¿ç”¨ä¸“å®¶ç­–ç•¥ç”Ÿæˆæ­£ç¡®åŠ¨ä½œ

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--expert_policy_file ./cs224r/policies/experts/Ant.pkl
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
loaded_expert_policy = LoadedGaussianPolicy(params['expert_policy_file'])
```

---

#### `--expert_data` / `-ed`
```python
parser.add_argument('--expert_data', '-ed', type=str, required=True)
```

**å«ä¹‰**ï¼šä¸“å®¶æ•°æ®æ–‡ä»¶çš„è·¯å¾„

**ä½œç”¨**ï¼š
- æŒ‡å®šé¢„æ”¶é›†çš„ä¸“å®¶è½¨è¿¹æ•°æ®ï¼ˆ`.pkl` æ–‡ä»¶ï¼‰
- åŒ…å«ä¸“å®¶åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæ—¶çš„è§‚æµ‹-åŠ¨ä½œå¯¹
- è¿™æ˜¯ Behavior Cloning çš„è®­ç»ƒæ•°æ®æ¥æº

**æ•°æ®ç»“æ„**ï¼š
- æ–‡ä»¶åŒ…å«ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè½¨è¿¹ï¼ˆpathï¼‰
- æ¯ä¸ªè½¨è¿¹æ˜¯å­—å…¸ï¼ŒåŒ…å«ï¼š
  - `observation`: è§‚æµ‹åºåˆ—
  - `action`: åŠ¨ä½œåºåˆ—
  - `reward`: å¥–åŠ±åºåˆ—
  - ç­‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--expert_data ./cs224r/expert_data/expert_data_Ant-v4.pkl
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
trainer.run_training_loop(
    initial_expertdata=params['expert_data'],  # ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶åŠ è½½
    ...
)
```

---

#### `--env_name` / `-env`
```python
parser.add_argument('--env_name', '-env', type=str,
    help=f'choices: {", ".join(MJ_ENV_NAMES)}', required=True)
```

**å«ä¹‰**ï¼šè¦ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåç§°

**å¯é€‰å€¼**ï¼š
- `Ant-v4`ï¼šèš‚èšç¯å¢ƒï¼ˆ6æ¡è…¿ï¼‰
- `Walker2d-v4`ï¼šä¸¤è¶³è¡Œèµ°æœºå™¨äºº
- `HalfCheetah-v4`ï¼šåŠçŒè±¹ï¼ˆå¿«é€Ÿç§»åŠ¨ï¼‰
- `Hopper-v4`ï¼šå•è¶³è·³è·ƒæœºå™¨äºº

**ä½œç”¨**ï¼š
- æŒ‡å®šè®­ç»ƒå’Œè¯„ä¼°çš„ç¯å¢ƒ
- ä¸åŒç¯å¢ƒæœ‰ä¸åŒçš„è§‚æµ‹ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--env_name Ant-v4
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.env = gym.make(self.params['env_name'], **self.params['env_kwargs'])
```

---

#### `--exp_name` / `-exp`
```python
parser.add_argument('--exp_name', '-exp', type=str,
    default='pick an experiment name', required=True)
```

**å«ä¹‰**ï¼šå®éªŒåç§°

**ä½œç”¨**ï¼š
- ç”¨äºåˆ›å»ºæ—¥å¿—ç›®å½•ï¼ŒåŒºåˆ†ä¸åŒçš„å®éªŒ
- æ—¥å¿—ç›®å½•æ ¼å¼ï¼š`q1_<exp_name>_<env_name>_<timestamp>`
- ä¾‹å¦‚ï¼š`q1_bc_ant_Ant-v4_01-01-2026_10-00-01`

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--exp_name my_bc_experiment
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
logdir = logdir_prefix + args.exp_name + '_' + args.env_name + '_' + \
    time.strftime("%d-%m-%Y_%H-%M-%S")
```

---

### 2. è®­ç»ƒæ§åˆ¶å‚æ•°ï¼ˆTraining Controlï¼‰

#### `--do_dagger`
```python
parser.add_argument('--do_dagger', action='store_true')
```

**å«ä¹‰**ï¼šæ˜¯å¦å¯ç”¨ DAgger ç®—æ³•

**ä½œç”¨**ï¼š
- å¦‚æœæä¾›æ­¤å‚æ•°ï¼šä½¿ç”¨ DAgger ç®—æ³•ï¼ˆéœ€è¦ `n_iter > 1`ï¼‰
- å¦‚æœä¸æä¾›ï¼šä½¿ç”¨æ ‡å‡†çš„ Behavior Cloningï¼ˆ`n_iter = 1`ï¼‰

**DAgger vs BC**ï¼š
- **BC**ï¼šåªç”¨ä¸“å®¶æ•°æ®è®­ç»ƒä¸€æ¬¡
- **DAgger**ï¼šè¿­ä»£è®­ç»ƒï¼Œæ¯æ¬¡ç”¨å½“å‰ç­–ç•¥æ”¶é›†æ•°æ®ï¼Œç”¨ä¸“å®¶é‡æ–°æ ‡æ³¨

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
# å¯ç”¨ DAgger
--do_dagger

# ä¸å¯ç”¨ï¼ˆæ ‡å‡† BCï¼‰
# ä¸æä¾›æ­¤å‚æ•°å³å¯
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
if args.do_dagger:
    logdir_prefix = 'q2_'
    assert args.n_iter > 1  # DAgger éœ€è¦å¤šæ¬¡è¿­ä»£
else:
    logdir_prefix = 'q1_'
    assert args.n_iter == 1  # BC åªéœ€è¦ä¸€æ¬¡è¿­ä»£
```

---

#### `--n_iter` / `-n`
```python
parser.add_argument('--n_iter', '-n', type=int, default=1)
```

**å«ä¹‰**ï¼šè®­ç»ƒè¿­ä»£æ¬¡æ•°

**ä½œç”¨**ï¼š
- æ§åˆ¶è®­ç»ƒå¾ªç¯æ‰§è¡Œå¤šå°‘æ¬¡
- **BC**ï¼šé€šå¸¸ä¸º 1ï¼ˆåªç”¨ä¸“å®¶æ•°æ®è®­ç»ƒä¸€æ¬¡ï¼‰
- **DAgger**ï¼šéœ€è¦ > 1ï¼ˆå¤šæ¬¡è¿­ä»£æ”¶é›†æ–°æ•°æ®å¹¶è®­ç»ƒï¼‰

**æ¨èå€¼**ï¼š
- BC: `1`
- DAgger: `10-20`ï¼ˆæ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--n_iter 1      # BC
--n_iter 10     # DAgger
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
for itr in range(n_iter):  # è®­ç»ƒå¾ªç¯
    # æ”¶é›†æ•°æ®ã€è®­ç»ƒã€è¯„ä¼°...
```

---

#### `--num_agent_train_steps_per_iter`
```python
parser.add_argument('--num_agent_train_steps_per_iter', type=int, default=1000)
```

**å«ä¹‰**ï¼šæ¯ä¸ªè¿­ä»£ä¸­ï¼ŒAgent è®­ç»ƒçš„æ¢¯åº¦æ­¥æ•°

**ä½œç”¨**ï¼š
- æ§åˆ¶æ¯ä¸ªè¿­ä»£ä¸­æ‰§è¡Œå¤šå°‘æ¬¡æ¢¯åº¦æ›´æ–°
- æ¯æ¬¡æ¢¯åº¦æ›´æ–°ä¼šä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®

**ç†è§£**ï¼š
- å‡è®¾ `n_iter=10`ï¼Œ`num_agent_train_steps_per_iter=1000`
- æ€»å…±ä¼šæ‰§è¡Œ `10 Ã— 1000 = 10,000` æ¬¡æ¢¯åº¦æ›´æ–°

**æ¨èå€¼**ï¼š
- å°ä»»åŠ¡ï¼š`500-1000`
- å¤§ä»»åŠ¡ï¼š`1000-2000`

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--num_agent_train_steps_per_iter 1000
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
for train_step in range(self.params['num_agent_train_steps_per_iter']):
    ob_batch, ac_batch, ... = self.agent.sample(...)
    train_log = self.agent.train(ob_batch, ac_batch)
```

---

### 3. æ•°æ®æ”¶é›†å‚æ•°ï¼ˆData Collectionï¼‰

#### `--batch_size`
```python
parser.add_argument('--batch_size', type=int, default=1000)
```

**å«ä¹‰**ï¼šæ¯æ¬¡è¿­ä»£æ”¶é›†çš„è®­ç»ƒæ•°æ®æ­¥æ•°ï¼ˆç¯å¢ƒäº¤äº’æ­¥æ•°ï¼‰

**ä½œç”¨**ï¼š
- æ§åˆ¶æ¯æ¬¡è¿­ä»£ä»ç¯å¢ƒä¸­æ”¶é›†å¤šå°‘æ­¥æ•°æ®
- ç”¨äº DAggerï¼šæ¯æ¬¡è¿­ä»£ç”¨å½“å‰ç­–ç•¥æ”¶é›†æ–°æ•°æ®
- æ³¨æ„ï¼šè¿™æ˜¯**ç¯å¢ƒæ­¥æ•°**ï¼Œä¸æ˜¯æ‰¹æ¬¡å¤§å°

**æ¨èå€¼**ï¼š
- å¼€å‘/è°ƒè¯•ï¼š`1000`ï¼ˆå¿«é€Ÿï¼‰
- æœ€ç»ˆç»“æœï¼š`â‰¥ 10,000`ï¼ˆæ›´ç¨³å®šï¼Œå‡å°‘æ–¹å·®ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--batch_size 10000
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
paths, envsteps_this_batch = utils.sample_trajectories(
    self.env, 
    collect_policy, 
    self.params['batch_size'],  # æ”¶é›†è¿™ä¹ˆå¤šæ­¥
    self.params['ep_len']
)
```

---

#### `--eval_batch_size`
```python
parser.add_argument('--eval_batch_size', type=int, default=1000)
```

**å«ä¹‰**ï¼šè¯„ä¼°æ—¶æ”¶é›†çš„æ•°æ®æ­¥æ•°

**ä½œç”¨**ï¼š
- æ§åˆ¶è¯„ä¼°æ—¶æ”¶é›†å¤šå°‘æ­¥æ•°æ®æ¥è®¡ç®—æ€§èƒ½æŒ‡æ ‡
- ç”¨äºè®¡ç®—å¹³å‡å›æŠ¥ã€æ ‡å‡†å·®ç­‰ç»Ÿè®¡é‡

**æ¨èå€¼**ï¼š
- `1000-5000`ï¼ˆè¶³å¤Ÿè®¡ç®—å¯é çš„ç»Ÿè®¡é‡ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--eval_batch_size 2000
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
eval_paths, eval_envsteps_this_batch = utils.sample_trajectories(
    self.env, eval_policy, self.params['eval_batch_size'],
    self.params['ep_len']
)
```

---

#### `--train_batch_size`
```python
parser.add_argument('--train_batch_size', type=int, default=100)
```

**å«ä¹‰**ï¼šæ¯æ¬¡æ¢¯åº¦æ›´æ–°æ—¶é‡‡æ ·çš„æ•°æ®ç‚¹æ•°é‡

**ä½œç”¨**ï¼š
- æ§åˆ¶æ¯æ¬¡æ¢¯åº¦æ›´æ–°ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·å¤šå°‘æ¡æ•°æ®
- è¿™æ˜¯çœŸæ­£çš„"æ‰¹æ¬¡å¤§å°"ï¼ˆbatch sizeï¼‰

**ç†è§£**ï¼š
- `batch_size=10000`ï¼šæ”¶é›† 10,000 æ­¥æ•°æ®åˆ°ç¼“å†²åŒº
- `train_batch_size=100`ï¼šæ¯æ¬¡æ¢¯åº¦æ›´æ–°ä»ç¼“å†²åŒºé‡‡æ · 100 æ¡æ•°æ®
- `num_agent_train_steps_per_iter=1000`ï¼šæ‰§è¡Œ 1000 æ¬¡æ¢¯åº¦æ›´æ–°
- æ€»å…±ä½¿ç”¨ï¼š`100 Ã— 1000 = 100,000` æ¡æ•°æ®ï¼ˆå¯èƒ½é‡å¤é‡‡æ ·ï¼‰

**æ¨èå€¼**ï¼š
- `32-256`ï¼ˆæ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--train_batch_size 128
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
ob_batch, ac_batch, ... = self.agent.sample(self.params['train_batch_size'])
```

---

#### `--ep_len`
```python
parser.add_argument('--ep_len', type=int)
```

**å«ä¹‰**ï¼šæ¯ä¸ªè½¨è¿¹ï¼ˆepisodeï¼‰çš„æœ€å¤§é•¿åº¦

**ä½œç”¨**ï¼š
- é™åˆ¶å•ä¸ªè½¨è¿¹çš„æœ€å¤§æ­¥æ•°
- å¦‚æœè½¨è¿¹æå‰ç»“æŸï¼ˆdone=Trueï¼‰ï¼Œåˆ™å®é™…é•¿åº¦å¯èƒ½æ›´çŸ­
- å¦‚æœä¸æä¾›ï¼Œä½¿ç”¨ç¯å¢ƒçš„é»˜è®¤æœ€å¤§é•¿åº¦

**æ¨èå€¼**ï¼š
- é€šå¸¸ä½¿ç”¨ç¯å¢ƒé»˜è®¤å€¼ï¼ˆä¸æä¾›æ­¤å‚æ•°ï¼‰
- å¦‚æœéœ€è¦é™åˆ¶ï¼š`200-1000`ï¼ˆæ ¹æ®ç¯å¢ƒè°ƒæ•´ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--ep_len 500
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.params['ep_len'] = self.params['ep_len'] or self.env.spec.max_episode_steps
# å¦‚æœä¸æä¾›ï¼Œä½¿ç”¨ç¯å¢ƒçš„é»˜è®¤å€¼
```

---

### 4. ç½‘ç»œæ¶æ„å‚æ•°ï¼ˆNetwork Architectureï¼‰

#### `--n_layers`
```python
parser.add_argument('--n_layers', type=int, default=2)
```

**å«ä¹‰**ï¼šç­–ç•¥ç½‘ç»œçš„éšè—å±‚æ•°é‡

**ä½œç”¨**ï¼š
- æ§åˆ¶ç­–ç•¥ç½‘ç»œçš„æ·±åº¦
- ä¸åŒ…æ‹¬è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ï¼Œåªè®¡ç®—éšè—å±‚

**ç½‘ç»œç»“æ„**ï¼š
- `n_layers=2`ï¼šè¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ è¾“å‡ºå±‚
- æ€»å…± 4 å±‚ï¼ˆ2 ä¸ªéšè—å±‚ + è¾“å…¥ + è¾“å‡ºï¼‰

**æ¨èå€¼**ï¼š
- ç®€å•ä»»åŠ¡ï¼š`2`
- å¤æ‚ä»»åŠ¡ï¼š`3-4`

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--n_layers 3
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.actor = MLPPolicySL(
    ...
    n_layers=self.agent_params['n_layers'],
    ...
)
```

---

#### `--size`
```python
parser.add_argument('--size', type=int, default=64)
```

**å«ä¹‰**ï¼šæ¯ä¸ªéšè—å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼ˆå®½åº¦ï¼‰

**ä½œç”¨**ï¼š
- æ§åˆ¶ç­–ç•¥ç½‘ç»œçš„å®½åº¦
- æ‰€æœ‰éšè—å±‚ä½¿ç”¨ç›¸åŒçš„å®½åº¦

**æ¨èå€¼**ï¼š
- å°ä»»åŠ¡ï¼š`32-64`
- ä¸­ç­‰ä»»åŠ¡ï¼š`64-128`
- å¤§ä»»åŠ¡ï¼š`128-256`

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--size 128
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.actor = MLPPolicySL(
    ...
    size=self.agent_params['size'],
    ...
)
```

---

### 5. ä¼˜åŒ–å™¨å‚æ•°ï¼ˆOptimizerï¼‰

#### `--learning_rate` / `-lr`
```python
parser.add_argument('--learning_rate', '-lr', type=float, default=5e-3)
```

**å«ä¹‰**ï¼šå­¦ä¹ ç‡

**ä½œç”¨**ï¼š
- æ§åˆ¶æ¢¯åº¦æ›´æ–°çš„æ­¥é•¿
- å¤ªå¤§ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œå¯èƒ½å‘æ•£
- å¤ªå°ï¼šè®­ç»ƒæ…¢ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

**æ¨èå€¼**ï¼š
- ç›‘ç£å­¦ä¹ ï¼ˆBCï¼‰ï¼š`1e-3` åˆ° `1e-2`ï¼ˆ0.001 åˆ° 0.01ï¼‰
- é»˜è®¤å€¼ `5e-3`ï¼ˆ0.005ï¼‰æ˜¯ä¸€ä¸ªä¸é”™çš„èµ·ç‚¹

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--learning_rate 0.001
# æˆ–
-lr 1e-3
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.actor = MLPPolicySL(
    ...
    learning_rate=self.agent_params['learning_rate'],
    ...
)
```

---

### 6. æ—¥å¿—å‚æ•°ï¼ˆLoggingï¼‰

#### `--video_log_freq`
```python
parser.add_argument('--video_log_freq', type=int, default=5)
```

**å«ä¹‰**ï¼šæ¯éš”å¤šå°‘æ¬¡è¿­ä»£è®°å½•ä¸€æ¬¡è§†é¢‘

**ä½œç”¨**ï¼š
- æ§åˆ¶è§†é¢‘è®°å½•çš„é¢‘ç‡
- è§†é¢‘ç”¨äºå¯è§†åŒ–ç­–ç•¥çš„è¡Œä¸º
- è®¾ç½®ä¸º `-1` å¯ä»¥ç¦ç”¨è§†é¢‘è®°å½•

**æ¨èå€¼**ï¼š
- å¼€å‘é˜¶æ®µï¼š`5-10`ï¼ˆæ›´é¢‘ç¹ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼‰
- æœ€ç»ˆè®­ç»ƒï¼š`10-20`ï¼ˆå‡å°‘å¼€é”€ï¼‰
- æ— æ˜¾ç¤ºç¯å¢ƒï¼š`-1`ï¼ˆç¦ç”¨ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--video_log_freq 10
--video_log_freq -1  # ç¦ç”¨è§†é¢‘
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
if itr % self.params['video_log_freq'] == 0 and self.params['video_log_freq'] != -1:
    self.log_video = True
```

---

#### `--scalar_log_freq`
```python
parser.add_argument('--scalar_log_freq', type=int, default=1)
```

**å«ä¹‰**ï¼šæ¯éš”å¤šå°‘æ¬¡è¿­ä»£è®°å½•ä¸€æ¬¡æ ‡é‡æŒ‡æ ‡

**ä½œç”¨**ï¼š
- æ§åˆ¶æŒ‡æ ‡è®°å½•çš„é¢‘ç‡
- æŒ‡æ ‡åŒ…æ‹¬ï¼šå¹³å‡å›æŠ¥ã€æŸå¤±ã€è®­ç»ƒæ­¥æ•°ç­‰

**æ¨èå€¼**ï¼š
- é€šå¸¸è®¾ä¸º `1`ï¼ˆæ¯æ¬¡éƒ½è®°å½•ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--scalar_log_freq 1
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
if itr % self.params['scalar_log_freq'] == 0:
    self.log_metrics = True
```

---

#### `--save_params`
```python
parser.add_argument('--save_params', action='store_true')
```

**å«ä¹‰**ï¼šæ˜¯å¦ä¿å­˜æ¨¡å‹å‚æ•°

**ä½œç”¨**ï¼š
- å¦‚æœæä¾›æ­¤å‚æ•°ï¼Œä¼šåœ¨æ¯æ¬¡è®°å½•æ—¥å¿—æ—¶ä¿å­˜æ¨¡å‹
- ä¿å­˜çš„æ¨¡å‹å¯ä»¥ç”¨äºåç»­è¯„ä¼°æˆ–ç»§ç»­è®­ç»ƒ

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--save_params
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
if self.params['save_params']:
    self.agent.save('{}/policy_itr_{}.pt'.format(self.params['logdir'], itr))
```

---

### 7. ç¡¬ä»¶å‚æ•°ï¼ˆHardwareï¼‰

#### `--no_gpu` / `-ngpu`
```python
parser.add_argument('--no_gpu', '-ngpu', action='store_true')
```

**å«ä¹‰**ï¼šæ˜¯å¦ç¦ç”¨ GPU

**ä½œç”¨**ï¼š
- å¦‚æœæä¾›æ­¤å‚æ•°ï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU
- å¦åˆ™å°è¯•ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰

**ä½¿ç”¨åœºæ™¯**ï¼š
- è°ƒè¯•æ—¶å¯èƒ½æƒ³ç”¨ CPUï¼ˆæ›´æ…¢ä½†æ›´ç¨³å®šï¼‰
- æ²¡æœ‰ GPU çš„æœºå™¨

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--no_gpu
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
ptu.init_gpu(
    use_gpu=not self.params['no_gpu'],  # å¦‚æœ no_gpu=Trueï¼Œåˆ™ use_gpu=False
    gpu_id=self.params['which_gpu']
)
```

---

#### `--which_gpu`
```python
parser.add_argument('--which_gpu', type=int, default=0)
```

**å«ä¹‰**ï¼šä½¿ç”¨å“ªä¸ª GPUï¼ˆåœ¨å¤š GPU ç³»ç»Ÿä¸­ï¼‰

**ä½œç”¨**ï¼š
- åœ¨å¤š GPU ç³»ç»Ÿä¸­æŒ‡å®šä½¿ç”¨å“ªä¸ª GPU
- GPU ç¼–å·ä» 0 å¼€å§‹

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--which_gpu 0  # ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU
--which_gpu 1  # ä½¿ç”¨ç¬¬äºŒä¸ª GPU
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
ptu.init_gpu(
    use_gpu=not self.params['no_gpu'],
    gpu_id=self.params['which_gpu']
)
```

---

### 8. å…¶ä»–å‚æ•°ï¼ˆMiscellaneousï¼‰

#### `--max_replay_buffer_size`
```python
parser.add_argument('--max_replay_buffer_size', type=int, default=1000000)
```

**å«ä¹‰**ï¼šç»éªŒå›æ”¾ç¼“å†²åŒºçš„æœ€å¤§å®¹é‡

**ä½œç”¨**ï¼š
- é™åˆ¶ç¼“å†²åŒºèƒ½å­˜å‚¨çš„æœ€å¤§æ•°æ®é‡
- è¶…è¿‡å®¹é‡æ—¶ï¼Œæ—§æ•°æ®ä¼šè¢«æ–°æ•°æ®è¦†ç›–ï¼ˆFIFOï¼‰

**æ¨èå€¼**ï¼š
- å°ä»»åŠ¡ï¼š`100,000 - 1,000,000`
- å¤§ä»»åŠ¡ï¼š`1,000,000 - 10,000,000`

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--max_replay_buffer_size 2000000
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
self.replay_buffer = ReplayBuffer(
    self.agent_params['max_replay_buffer_size']
)
```

---

#### `--seed`
```python
parser.add_argument('--seed', type=int, default=1)
```

**å«ä¹‰**ï¼šéšæœºç§å­

**ä½œç”¨**ï¼š
- è®¾ç½®éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
- ç›¸åŒçš„ç§å­ä¼šäº§ç”Ÿç›¸åŒçš„ç»“æœ

**æ¨èå€¼**ï¼š
- å¼€å‘ï¼š`1`
- å®éªŒï¼šä½¿ç”¨ä¸åŒçš„ç§å­å¤šæ¬¡è¿è¡Œï¼Œå–å¹³å‡

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
--seed 42
```

**ä»£ç ä¸­çš„ä½¿ç”¨**ï¼š
```python
seed = self.params['seed']
np.random.seed(seed)
torch.manual_seed(seed)
self.env.reset(seed=seed)
```

---

## ğŸ“Š å‚æ•°å…³ç³»å›¾

```
è®­ç»ƒæµç¨‹ä¸­çš„å‚æ•°å…³ç³»ï¼š

n_iter (è¿­ä»£æ¬¡æ•°)
  â””â”€> æ¯æ¬¡è¿­ä»£ï¼š
      â”œâ”€> batch_size (æ”¶é›†æ•°æ®æ­¥æ•°)
      â”‚   â””â”€> æ”¶é›†åˆ°ç¼“å†²åŒº
      â”‚
      â”œâ”€> num_agent_train_steps_per_iter (æ¢¯åº¦æ›´æ–°æ¬¡æ•°)
      â”‚   â””â”€> æ¯æ¬¡æ›´æ–°ï¼š
      â”‚       â””â”€> train_batch_size (é‡‡æ ·æ‰¹æ¬¡å¤§å°)
      â”‚
      â””â”€> eval_batch_size (è¯„ä¼°æ•°æ®æ­¥æ•°)
          â””â”€> è®¡ç®—æ€§èƒ½æŒ‡æ ‡
```

---

## ğŸ¯ å¸¸ç”¨å‚æ•°ç»„åˆç¤ºä¾‹

### Behavior Cloningï¼ˆæ ‡å‡†ï¼‰
```bash
python run_hw1.py \
    --expert_policy_file ./cs224r/policies/experts/Ant.pkl \
    --expert_data ./cs224r/expert_data/expert_data_Ant-v4.pkl \
    --env_name Ant-v4 \
    --exp_name bc_ant \
    --n_iter 1 \
    --batch_size 10000 \
    --train_batch_size 128 \
    --learning_rate 0.001 \
    --n_layers 2 \
    --size 64
```

### DAgger
```bash
python run_hw1.py \
    --expert_policy_file ./cs224r/policies/experts/Ant.pkl \
    --expert_data ./cs224r/expert_data/expert_data_Ant-v4.pkl \
    --env_name Ant-v4 \
    --exp_name dagger_ant \
    --do_dagger \
    --n_iter 10 \
    --batch_size 10000 \
    --num_agent_train_steps_per_iter 1000 \
    --train_batch_size 128 \
    --learning_rate 0.001 \
    --n_layers 2 \
    --size 64
```

---

## ğŸ’¡ å‚æ•°è°ƒä¼˜å»ºè®®

1. **å…ˆä»å°å‚æ•°å¼€å§‹**ï¼š`batch_size=1000`, `train_batch_size=32`ï¼Œå¿«é€ŸéªŒè¯ä»£ç 
2. **é€æ­¥å¢å¤§**ï¼šç¡®è®¤ä»£ç æ­£ç¡®åï¼Œå¢å¤§ `batch_size` åˆ° `10000+` è·å¾—æ›´å¥½ç»“æœ
3. **å­¦ä¹ ç‡**ï¼šä»é»˜è®¤å€¼å¼€å§‹ï¼Œå¦‚æœæŸå¤±ä¸ä¸‹é™ï¼Œå°è¯• `1e-4` æˆ– `1e-2`
4. **ç½‘ç»œå¤§å°**ï¼šç®€å•ä»»åŠ¡ç”¨ `n_layers=2, size=64`ï¼Œå¤æ‚ä»»åŠ¡ç”¨ `n_layers=3, size=128`
5. **å¤šæ¬¡è¿è¡Œ**ï¼šä½¿ç”¨ä¸åŒ `seed` å¤šæ¬¡è¿è¡Œï¼Œå–å¹³å‡ç»“æœ

---

## ğŸ” å¿«é€Ÿå‚è€ƒè¡¨

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ç”¨é€” |
|------|------|--------|------|
| `--expert_policy_file` | str | å¿…éœ€ | ä¸“å®¶ç­–ç•¥æ–‡ä»¶è·¯å¾„ |
| `--expert_data` | str | å¿…éœ€ | ä¸“å®¶æ•°æ®æ–‡ä»¶è·¯å¾„ |
| `--env_name` | str | å¿…éœ€ | ç¯å¢ƒåç§° |
| `--exp_name` | str | å¿…éœ€ | å®éªŒåç§° |
| `--do_dagger` | flag | False | å¯ç”¨ DAgger |
| `--n_iter` | int | 1 | è¿­ä»£æ¬¡æ•° |
| `--batch_size` | int | 1000 | æ”¶é›†æ•°æ®æ­¥æ•° |
| `--train_batch_size` | int | 100 | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| `--n_layers` | int | 2 | ç½‘ç»œå±‚æ•° |
| `--size` | int | 64 | éšè—å±‚å®½åº¦ |
| `--learning_rate` | float | 0.005 | å­¦ä¹ ç‡ |
| `--seed` | int | 1 | éšæœºç§å­ |

